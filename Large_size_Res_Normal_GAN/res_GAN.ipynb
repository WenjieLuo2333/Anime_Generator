{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "import matplotlib.gridspec as gridspec\n",
    "from random import sample\n",
    "import random\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Activation, Add, Dropout,Reshape, Concatenate,PReLU,Embedding,multiply\n",
    "from keras.layers.core import Dense,Activation,Flatten\n",
    "from keras.layers import UpSampling2D, Lambda\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Model,Sequential\n",
    "from keras.layers.convolutional import Conv2D,UpSampling2D,Conv2DTranspose\n",
    "from keras.layers.pooling import AvgPool2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.image import load_img\n",
    "from skimage.transform import resize\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ACGAN():\n",
    "    def __init__(self):\n",
    "        # Input shape\n",
    "        self.img_rows = 64\n",
    "        self.img_cols = 64\n",
    "        self.channels = 3\n",
    "        self.img_shape = (self.img_rows, self.img_cols,self.channels)\n",
    "        self.num_classes = 12\n",
    "        self.latent_dim = 100\n",
    "\n",
    "        optimizer1 = Adam(0.0002, 0.5)\n",
    "        optimizer2 = Adam(0.00003, 0.5)\n",
    "        losses = ['binary_crossentropy']\n",
    "              \n",
    "        # Build and compile the discriminator\n",
    "        img_input = Input(shape=(self.img_rows,self.img_cols,self.channels))\n",
    "        self.discriminator,valid = self.get_discriminator(img_input)\n",
    "        self.discriminator.compile(loss=losses,\n",
    "            optimizer=optimizer2,\n",
    "            metrics=['accuracy'])      \n",
    "\n",
    "\n",
    "        # Build the generator\n",
    "        noise = Input(shape=(100,))\n",
    "\n",
    "        # The generator takes noise and the target label as input\n",
    "        # and generates the corresponding digit of that label\n",
    "        self.generator = self.get_generator(noise)\n",
    "        img = self.generator(noise)\n",
    "        valid = self.discriminator(img)\n",
    "\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated image as input and determines validity\n",
    "        # and the label of that image\n",
    "       \n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(noise,valid)\n",
    "        self.combined.compile(loss=losses,\n",
    "            optimizer=optimizer1,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "    \"\"\"    \n",
    "    def SubpixelConv2D(self,name, scale=2):\n",
    "        def subpixel_shape(input_shape):\n",
    "            dims = [input_shape[0],\n",
    "                    None if input_shape[1] is None else input_shape[1] * scale,\n",
    "                    None if input_shape[2] is None else input_shape[2] * scale,\n",
    "                    int(input_shape[3] / (scale ** 2))]\n",
    "            output_shape = tuple(dims)\n",
    "            return output_shape\n",
    "\n",
    "        def subpixel(x):\n",
    "            return tf.depth_to_space(x, scale)\n",
    "\n",
    "        return Lambda(subpixel, output_shape=subpixel_shape, name=name)\n",
    "    \"\"\"\n",
    "    def get_generator(self,input_layer):\n",
    "        def residual_block(input):\n",
    "            x = Conv2D(64, kernel_size=3, strides=1, padding='same')(input)\n",
    "            x = BatchNormalization(momentum=0.8)(x)\n",
    "            x = LeakyReLU(0.2)(x)            \n",
    "            x = Conv2D(64, kernel_size=3, strides=1, padding='same')(x)\n",
    "            x = BatchNormalization(momentum=0.8)(x)\n",
    "            #x = Add()([x, input])\n",
    "            return x\n",
    "\n",
    "        def upsample(x, number):\n",
    "            x = Conv2D(256, kernel_size=3, strides=1, padding='same', name='upSampleConv2d_'+str(number))(x)\n",
    "            x = self.SubpixelConv2D('upSampleSubPixel_'+str(number), 2)(x)\n",
    "            x = PReLU(shared_axes=[1,2], name='upSamplePReLU_'+str(number))(x)\n",
    "            return x\n",
    "\n",
    "        hid = Dense(512 * 8 * 8, activation='relu')(input_layer)    \n",
    "        x_start = Reshape((8, 8, 512))(hid)\n",
    "        \n",
    "       \n",
    "        gen = Conv2D(256, (3,3), padding='same')(x_start)\n",
    "        gen = BatchNormalization(momentum = 0.5)(gen)\n",
    "        gen = LeakyReLU(0.2)(gen)\n",
    "        # gen = Activation('ReLU')(gen)\n",
    "\n",
    "        gen = UpSampling2D(size=(2,2))(gen)\n",
    "        gen = Conv2D(128, (3,3), padding='same')(gen)\n",
    "        gen = BatchNormalization(momentum = 0.5)(gen)\n",
    "        gen = LeakyReLU(0.2)(gen)\n",
    "        # gen = Activation('ReLU')(gen)\n",
    "        gen = Conv2D(64, (3,3), padding='same')(gen)\n",
    "        gen = BatchNormalization(momentum = 0.5)(gen)\n",
    "        gen = LeakyReLU(0.2)(gen)\n",
    "\n",
    "        gen = UpSampling2D(size=(2,2))(gen)\n",
    "        gen = Conv2D(64, (3,3), padding='same')(gen)\n",
    "        gen = BatchNormalization(momentum = 0.5)(gen)\n",
    "        gen = LeakyReLU(0.2)(gen)\n",
    "        gen = Conv2D(64, (3,3), padding='same')(gen)\n",
    "        gen = BatchNormalization(momentum = 0.5)(gen)\n",
    "        gen = LeakyReLU(0.2)(gen)\n",
    "        gen = UpSampling2D(size=(2,2))(gen)\n",
    "        gen = Conv2D(64, (3,3), padding='same')(gen)\n",
    "        gen = BatchNormalization(momentum = 0.5)(gen)\n",
    "        gen = LeakyReLU(0.2)(gen)\n",
    "        gen = Conv2D(64, (3,3), padding='same')(gen)\n",
    "        gen = BatchNormalization(momentum = 0.5)(gen)\n",
    "        gen = LeakyReLU(0.2)(gen)\n",
    "        #gen = upsample(gen,1)\n",
    "        #gen = upsample(gen,2)\n",
    "        #gen = upsample(gen,3)\n",
    "        gen = Conv2D(3, (3,3), padding='same')(gen)\n",
    "        gen = Activation('tanh')(gen)\n",
    "    \n",
    "        \"\"\"hid = Dense(512 * 12 * 12, activation='relu')(input_layer)    \n",
    "        x_start = Reshape((12, 12, 512))(hid)\n",
    "        \n",
    "       \n",
    "        gen = Conv2D(256, (3,3), padding='same')(x_start)\n",
    "        gen = BatchNormalization(momentum = 0.5)(gen)\n",
    "        gen = LeakyReLU(0.2)(gen)\n",
    "        # gen = Activation('ReLU')(gen)\n",
    "\n",
    "        gen = UpSampling2D(size=(2,2))(gen)\n",
    "        gen = Conv2D(128, (3,3), padding='same')(gen)\n",
    "        gen = BatchNormalization(momentum = 0.5)(gen)\n",
    "        gen = LeakyReLU(0.2)(gen)\n",
    "        # gen = Activation('ReLU')(gen)\n",
    "        gen = Conv2D(64, (3,3), padding='same')(gen)\n",
    "        gen = BatchNormalization(momentum = 0.5)(gen)\n",
    "        gen = LeakyReLU(0.2)(gen)\n",
    "\n",
    "        gen = UpSampling2D(size=(2,2))(gen)\n",
    "        gen = Conv2D(64, (3,3), padding='same')(gen)\n",
    "        gen = BatchNormalization(momentum = 0.5)(gen)\n",
    "        gen = LeakyReLU(0.2)(gen)\n",
    "        gen = Conv2D(64, (3,3), padding='same')(gen)\n",
    "        gen = BatchNormalization(momentum = 0.5)(gen)\n",
    "        gen = LeakyReLU(0.2)(gen)\n",
    "        gen = UpSampling2D(size=(2,2))(gen)\n",
    "        gen = Conv2D(64, (3,3), padding='same')(gen)\n",
    "        gen = BatchNormalization(momentum = 0.5)(gen)\n",
    "        gen = LeakyReLU(0.2)(gen)\n",
    "        gen = Conv2D(64, (3,3), padding='same')(gen)\n",
    "        gen = BatchNormalization(momentum = 0.5)(gen)\n",
    "        gen = LeakyReLU(0.2)(gen)\n",
    "        #gen = upsample(gen,1)\n",
    "        #gen = upsample(gen,2)\n",
    "        #gen = upsample(gen,3)\n",
    "        gen = Conv2D(3, (3,3), padding='same')(gen)\n",
    "        gen = Activation('tanh')(gen)\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"  \n",
    "        x_start = Conv2D(64, kernel_size=9, strides=1, padding='same')(x_start)\n",
    "        x_start = PReLU(shared_axes=[1,2])(x_start)\n",
    "        r = residual_block(x_start)\n",
    "        #for _ in range(3):\n",
    "        #    r = residual_block(r)\n",
    "        x = Conv2D(64, kernel_size=3, strides=1, padding='same')(r)\n",
    "        x = BatchNormalization(momentum=0.8)(x)\n",
    "        x = Add()([x, x_start])\n",
    "        \n",
    "        \n",
    "        #gen = UpSampling2D(size=(2,2))(x)\n",
    "        gen = Conv2D(256, (3,3), padding='same')(x)\n",
    "        gen = BatchNormalization(momentum = 0.5)(gen)\n",
    "        gen = LeakyReLU(0.2)(gen)   \n",
    "        \n",
    "        gen = residual_block(gen) \n",
    "        gen = upsample(gen, 1)\n",
    "        #gen = UpSampling2D(size=(2,2))(gen)\n",
    "        gen = Conv2D(256, (3,3), padding='same')(gen)\n",
    "        gen = BatchNormalization(momentum = 0.5)(gen)\n",
    "        gen = LeakyReLU(0.2)(gen) \n",
    "        \n",
    "        gen = upsample(gen, 2)\n",
    "        gen = residual_block(gen) \n",
    "        gen = residual_block(gen) \n",
    "        gen = upsample(gen, 3)\n",
    "        \n",
    "        #gen = UpSampling2D(size=(2,2))(gen)\n",
    "        gen = Conv2D(256, (3,3), padding='same')(gen)\n",
    "        gen = BatchNormalization(momentum = 0.5)(gen)\n",
    "        gen = LeakyReLU(0.2)(gen) \n",
    "        \n",
    "        gen = residual_block(gen) \n",
    "        gen = residual_block(gen) \n",
    "        \"\"\"\n",
    "\n",
    "        #x = upsample(x, 1)\n",
    "        #x = upsample(x, 2)\n",
    "        #x = upsample(x, 3)\n",
    "        hr_output = Conv2D(\n",
    "                3, \n",
    "                kernel_size=9, \n",
    "                strides=1, \n",
    "                padding='same', \n",
    "                activation='tanh'\n",
    "            )(gen)\n",
    "        model = Model(inputs=input_layer, outputs=hr_output)  \n",
    "        model.summary()\n",
    "        return model\n",
    "    \n",
    "\n",
    "    def get_discriminator(self,input_layer):\n",
    "            def conv2d_block(input, filters, strides=1, bn=True):\n",
    "                d = Conv2D(filters, kernel_size=3, strides=strides, padding='same')(input)\n",
    "                d = LeakyReLU(alpha=0.2)(d)\n",
    "                if bn:\n",
    "                    d = BatchNormalization(momentum=0.8)(d)\n",
    "                return d\n",
    "            def residual_block(input,filters=64):\n",
    "                x = Conv2D(filters, kernel_size=3, strides=1, padding='same')(input)\n",
    "                x = BatchNormalization(momentum=0.8)(x)\n",
    "                x = PReLU(shared_axes=[1,2])(x)            \n",
    "                x = Conv2D(filters, kernel_size=3, strides=1, padding='same')(x)\n",
    "                x = BatchNormalization(momentum=0.8)(x)\n",
    "                x = Add()([x, input])\n",
    "                return x\n",
    "\n",
    "            filters = 32\n",
    "            x = Conv2D(filters, kernel_size=3, strides=2, padding='same')(input_layer)\n",
    "            x = LeakyReLU(alpha=0.2)(x)\n",
    "            x = residual_block(x,filters)\n",
    "            #x = residual_block(x,filters)\n",
    "            x = Conv2D(filters*4, kernel_size=3, strides=2, padding='same')(x)\n",
    "            x = LeakyReLU(alpha=0.2)(x)\n",
    "            x = residual_block(x, filters*4)\n",
    "            x = residual_block(x, filters*4)\n",
    "            x = Conv2D(filters*8, kernel_size=3, strides=2, padding='same')(x)\n",
    "            x = LeakyReLU(alpha=0.2)(x)\n",
    "            x = residual_block(x, filters*8)\n",
    "            x = residual_block(x, filters*8)\n",
    "            x = Conv2D(filters*8, kernel_size=3, strides=2, padding='same')(x)\n",
    "            x = LeakyReLU(alpha=0.2)(x)\n",
    "            x = residual_block(x, filters*8)\n",
    "            x = residual_block(x, filters*8)\n",
    "            x = Conv2D(filters*8, kernel_size=3, strides=2, padding='same')(x)\n",
    "            x = LeakyReLU(alpha=0.2)(x)\n",
    "            \"\"\"            \n",
    "            x = Conv2D(filters, kernel_size=3, strides=2, padding='same')(input_layer)\n",
    "            x = LeakyReLU(alpha=0.2)(x)\n",
    "            x = residual_block(x,filters)\n",
    "            #x = residual_block(x,filters)\n",
    "            x = Conv2D(filters*2, kernel_size=3, strides=2, padding='same')(x)\n",
    "            x = LeakyReLU(alpha=0.2)(x)\n",
    "            x = residual_block(x, filters*2)\n",
    "            x = residual_block(x, filters*2)\n",
    "            x = Conv2D(filters*4, kernel_size=3, strides=2, padding='same')(x)\n",
    "            x = LeakyReLU(alpha=0.2)(x)\n",
    "            x = residual_block(x, filters*4)\n",
    "            x = residual_block(x, filters*4)\n",
    "            x = Conv2D(filters*8, kernel_size=3, strides=2, padding='same')(x)\n",
    "            x = LeakyReLU(alpha=0.2)(x)\n",
    "            #x = residual_block(x, filters*8)\n",
    "            x = residual_block(x, filters*8)\n",
    "            x = Conv2D(filters*16, kernel_size=3, strides=2, padding='same')(x)\n",
    "            x = LeakyReLU(alpha=0.2)(x)\n",
    "            #x = residual_block(x, filters*8)\n",
    "            x = residual_block(x, filters*16)\n",
    "            x = Conv2D(filters*32, kernel_size=3, strides=2, padding='same')(x)\n",
    "            x = LeakyReLU(alpha=0.2)(x)\n",
    "            \"\"\"\n",
    "            x = Flatten()(x)\n",
    "            x1 = Dense(1, activation='sigmoid')(x)\n",
    "            \n",
    "            model = Model(inputs=input_layer, outputs=x1)\n",
    "            model.summary()\n",
    "            return model,x1\n",
    "    def train(self,X_train, epochs, batch_size=128, sample_interval=50,img_name=''):\n",
    "\n",
    "        # Load the dataset\n",
    "        #(X_train, y_train), (_, _) = mnist.load_data()\n",
    "\n",
    "        # Configure inputs\n",
    "        #X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "        #X_train = np.expand_dims(X_train, axis=3)\n",
    "        #y_train = y_train.reshape(-1, 1)\n",
    "        #print('y_train',y_train)\n",
    "        \n",
    "        # Adversarial ground truths\n",
    "        #self.generator.load_weights('./No_Res_Module/generator_weights.hdf5',by_name=True)\n",
    "        #self.discriminator.load_weights('./No_Res_Module/discriminator_weights.hdf5',by_name=True)\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "        pre_g_loss = 0\n",
    "        id_list = [_ for _ in range(X_train.shape[0])]\n",
    "        random.shuffle(id_list)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            for mini_batch in range(X_train.shape[0]//batch_size-2):\n",
    "                # ---------------------\n",
    "                #  Train Discriminator\n",
    "                # ---------------------\n",
    "\n",
    "                # Select a random batch of images\n",
    "                #idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "                idx = np.array([id_list[_] for _ in range(batch_size*mini_batch,batch_size*(mini_batch+1))])\n",
    "                imgs = X_train[idx]\n",
    "\n",
    "                # Sample noise as generator input\n",
    "                noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "                #print('imgs',np.shape(imgs))\n",
    "                # The labels of the digits that the generator tries to create an\n",
    "                # image representation of\n",
    "                #sampled_labels = np.random.randint(0, self.num_classes, (batch_size, 1))\n",
    "                # Generate a half batch of new images\n",
    "                gen_imgs = self.generator.predict(noise)\n",
    "                #print('G',np.shape(gen_imgs))\n",
    "                # Image labels. 0-11 if image is valid or 12 if it is generated (fake)\n",
    "                #img_labels = y_train[idx]\n",
    "                #fake_labels = self.num_classes * np.ones(img_labels.shape)\n",
    "                #print('D',(self.discriminator.predict(gen_imgs)))\n",
    "                # Train the discriminator\n",
    "                if mini_batch % 3 == 0 and pre_g_loss < 5:\n",
    "                    d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
    "                    #print('real_loss',d_loss_real)\n",
    "                    d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "                    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "                # ---------------------\n",
    "                #  Train Generator\n",
    "                # ---------------------\n",
    "\n",
    "                # Train the generator\n",
    "                g_loss = self.combined.train_on_batch(noise, valid)\n",
    "                pre_g_loss = g_loss[0]\n",
    "                # Plot the progress\n",
    "                #print('d_loss',d_loss)\n",
    "                #print('g_loss',g_loss)\n",
    "                print (\"Epoch%d,MiniBatch:%d [D loss: %f, acc.: %.2f%%] [G loss: %fï¼ŒG acc: %f]\" % (epoch,mini_batch, d_loss[0], 100*d_loss[1], g_loss[0],g_loss[1]))\n",
    "\n",
    "                # If at save interval => save generated image samples\n",
    "                if mini_batch % sample_interval == 1 or mini_batch == (X_train.shape[0]//batch_size-1):\n",
    "                    self.save_model()\n",
    "                    self.sample_images(epoch,mini_batch,img_name)\n",
    "            \n",
    "\n",
    "    def sample_images(self, epoch,mini_batch,img_name):\n",
    "        r, c = 2, 6\n",
    "        noise = np.random.normal(0, 1, (r * c, 100))\n",
    "        sampled_labels = np.array([_ for _ in range(r*c)])\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "        gen_imgs = self.combine_images(gen_imgs) \n",
    "        \"\"\"        \n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt,:,:,:])\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(\"gen_imgs/%d_%d.png\" % (epoch,mini_batch))\n",
    "        plt.close()\n",
    "        \"\"\"\n",
    "          \n",
    "        plt.imshow(gen_imgs)\n",
    "        plt.axis('off')\n",
    "        plt.savefig(\"gen_imgs/%s%d_%d.png\" % (img_name,epoch,mini_batch))\n",
    "        plt.close()\n",
    "        \n",
    "        \n",
    "    \n",
    "    def combine_images(self, generated_images):\n",
    "        num = generated_images.shape[0]\n",
    "        width = 6\n",
    "        height = 2\n",
    "        shape = generated_images.shape[1:3]\n",
    "        image = np.zeros((height*shape[0], width*shape[1],3),\n",
    "                         dtype=generated_images.dtype)\n",
    "        for index, img in enumerate(generated_images):\n",
    "            i = int(index/width)\n",
    "            j = index % width\n",
    "            image[i*shape[0]:(i+1)*shape[0], j*shape[1]:(j+1)*shape[1],:] = img[:, :, :]\n",
    "        return image\n",
    "    \n",
    "    def save_model(self):\n",
    "        def save(model, model_name):\n",
    "            model_path = \"No_Res_Module/%s.json\" % model_name\n",
    "            weights_path = \"No_Res_Module/%s_weights.hdf5\" % model_name\n",
    "            options = {\"file_arch\": model_path,\n",
    "                        \"file_weight\": weights_path}\n",
    "            json_string = model.to_json()\n",
    "            open(options['file_arch'], 'w').write(json_string)\n",
    "            model.save_weights(options['file_weight'])\n",
    "\n",
    "        save(self.generator, \"generator_extra\")\n",
    "        save(self.discriminator, \"discriminator_extra\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = pd.read_csv('./extra_data/extra_data/convert_tags.csv')\n",
    "X_train = np.array([(np.array(load_img(\"./extra_data/extra_data/images/{}.jpg\".format(ids))) / 127.5 - 1) for ids in tqdm_notebook(tags.idx)])\n",
    "np.shape(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = np.array([np.fliplr(x) for x in X_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acgan = ACGAN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "acgan.train(X_train,epochs=20, batch_size=32, sample_interval=200,img_name='extra_')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "X_train = np.array([np.fliplr(x) for x in X_train])\n",
    "acgan.train(X_train,epochs=20, batch_size=32, sample_interval=200,img_name='flip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
