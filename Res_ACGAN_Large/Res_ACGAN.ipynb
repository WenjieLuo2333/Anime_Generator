{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "import matplotlib.gridspec as gridspec\n",
    "from random import sample\n",
    "\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Activation, Add, Dropout,Reshape, Concatenate,PReLU,Embedding,multiply\n",
    "from keras.layers.core import Dense,Activation,Flatten\n",
    "from keras.layers import UpSampling2D, Lambda\n",
    "\n",
    "from keras.models import Model,Sequential\n",
    "from keras.layers.convolutional import Conv2D,UpSampling2D,Conv2DTranspose\n",
    "from keras.layers.pooling import AvgPool2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.image import load_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ACGAN():\n",
    "    \"\"\"\n",
    "    一共12类。定义了带Resnet的G和D，Loss Function是按照官方Keras的ACGAN选的。\n",
    "    之前有一次loss到了nan是因为最大类别数写错了。\n",
    "    现在的版本是在mnist上面的版本。\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Input shape\n",
    "        self.img_rows = 96\n",
    "        self.img_cols = 96\n",
    "        self.channels = 3\n",
    "        self.img_shape = (self.img_rows, self.img_cols,self.channels)\n",
    "        self.num_classes = 12\n",
    "        self.latent_dim = 100\n",
    "\n",
    "        optimizer1 = Adam(0.0002, 0.5)\n",
    "        optimizer2 = Adam(0.0002, 0.5)\n",
    "        losses = ['binary_crossentropy', 'sparse_categorical_crossentropy']\n",
    "        #losses = ['binary_crossentropy']\n",
    "        loss_weight=[1.,0.5]\n",
    "              \n",
    "        # Build and compile the discriminator\n",
    "        img_input = Input(shape=(self.img_rows,self.img_cols,self.channels))\n",
    "        self.discriminator,valid, target_label = self.get_discriminator(img_input)\n",
    "        self.discriminator.compile(loss=losses,\n",
    "            optimizer=optimizer2,\n",
    "            metrics=['accuracy'])      \n",
    "\n",
    "\n",
    "        # Build the generator\n",
    "        noise = Input(shape=(100,))\n",
    "        label = Input(shape=(1,))\n",
    "\n",
    "\n",
    "        # The generator takes noise and the target label as input\n",
    "        # and generates the corresponding digit of that label\n",
    "        self.generator = self.get_generator(noise,label)\n",
    "        img = self.generator([noise,label])\n",
    "        valid, target_label = self.discriminator(img)\n",
    "\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated image as input and determines validity\n",
    "        # and the label of that image\n",
    "       \n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model([noise,label],[valid, target_label])\n",
    "        self.combined.compile(loss=losses,loss_weights=loss_weight,\n",
    "            optimizer=optimizer1)\n",
    "        \n",
    "    def SubpixelConv2D(self,name, scale=2):\n",
    "        \"\"\"\n",
    "        Keras layer to do subpixel convolution.\n",
    "        NOTE: Tensorflow backend only. Uses tf.depth_to_space\n",
    "\n",
    "        :param scale: upsampling scale compared to input_shape. Default=2\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        def subpixel_shape(input_shape):\n",
    "            dims = [input_shape[0],\n",
    "                    None if input_shape[1] is None else input_shape[1] * scale,\n",
    "                    None if input_shape[2] is None else input_shape[2] * scale,\n",
    "                    int(input_shape[3] / (scale ** 2))]\n",
    "            output_shape = tuple(dims)\n",
    "            return output_shape\n",
    "\n",
    "        def subpixel(x):\n",
    "            return tf.depth_to_space(x, scale)\n",
    "\n",
    "        return Lambda(subpixel, output_shape=subpixel_shape, name=name)\n",
    "\n",
    "    def get_generator(self,input_layer, condition_layer):\n",
    "        def residual_block(input):\n",
    "            x = Conv2D(64, kernel_size=3, strides=1, padding='same')(input)\n",
    "            x = BatchNormalization(momentum=0.8)(x)\n",
    "            x = PReLU(shared_axes=[1,2])(x)            \n",
    "            x = Conv2D(64, kernel_size=3, strides=1, padding='same')(x)\n",
    "            x = BatchNormalization(momentum=0.8)(x)\n",
    "            x = Add()([x, input])\n",
    "            return x\n",
    "\n",
    "        def upsample(x, number):\n",
    "            x = Conv2D(256, kernel_size=3, strides=1, padding='same', name='upSampleConv2d_'+str(number))(x)\n",
    "            x = self.SubpixelConv2D('upSampleSubPixel_'+str(number), 2)(x)\n",
    "            x = PReLU(shared_axes=[1,2], name='upSamplePReLU_'+str(number))(x)\n",
    "            return x\n",
    "\n",
    "        \n",
    "        \n",
    "        label_embedding = Flatten()(Embedding(self.num_classes, 100)(condition_layer))\n",
    "        \n",
    "        model_input = Concatenate()([input_layer,label_embedding])\n",
    "\n",
    "        hid = Dense(512 * 12 * 12, activation='relu')(model_input)    \n",
    "        hid = BatchNormalization(momentum=0.9)(hid)\n",
    "        hid = LeakyReLU(alpha=0.1)(hid)\n",
    "        x_start = Reshape((12, 12, 512))(hid)\n",
    "\n",
    "        x_start = Conv2D(64, kernel_size=9, strides=1, padding='same')(x_start)\n",
    "        x_start = PReLU(shared_axes=[1,2])(x_start)\n",
    "        r = residual_block(x_start)\n",
    "        for _ in range(9):\n",
    "            r = residual_block(r)\n",
    "        x = Conv2D(64, kernel_size=3, strides=1, padding='same')(r)\n",
    "        x = BatchNormalization(momentum=0.8)(x)\n",
    "        x = Add()([x, x_start])\n",
    "        \n",
    "        gen = UpSampling2D(size=(2,2))(x)\n",
    "        gen = Conv2D(256, (3,3), padding='same')(gen)\n",
    "        gen = BatchNormalization(momentum = 0.5)(gen)\n",
    "        gen = LeakyReLU(0.2)(gen)   \n",
    "        \n",
    "        gen = UpSampling2D(size=(2,2))(gen)\n",
    "        gen = Conv2D(256, (3,3), padding='same')(gen)\n",
    "        gen = BatchNormalization(momentum = 0.5)(gen)\n",
    "        gen = LeakyReLU(0.2)(gen) \n",
    "        \n",
    "        gen = UpSampling2D(size=(2,2))(gen)\n",
    "        gen = Conv2D(256, (3,3), padding='same')(gen)\n",
    "        gen = BatchNormalization(momentum = 0.5)(gen)\n",
    "        gen = LeakyReLU(0.2)(gen) \n",
    "\n",
    "        #x = upsample(x, 1)\n",
    "        #x = upsample(x, 2)\n",
    "        #x = upsample(x, 3)\n",
    "        hr_output = Conv2D(\n",
    "                3, \n",
    "                kernel_size=9, \n",
    "                strides=1, \n",
    "                padding='same', \n",
    "                activation='tanh'\n",
    "            )(gen)\n",
    "        model = Model(inputs=[input_layer, condition_layer], outputs=hr_output)  \n",
    "        model.summary()\n",
    "        return model\n",
    "\n",
    "    def get_discriminator(self,input_layer):\n",
    "            def conv2d_block(input, filters, strides=1, bn=True):\n",
    "                d = Conv2D(filters, kernel_size=3, strides=strides, padding='same')(input)\n",
    "                d = LeakyReLU(alpha=0.2)(d)\n",
    "                if bn:\n",
    "                    d = BatchNormalization(momentum=0.8)(d)\n",
    "                return d\n",
    "            def residual_block(input,filters=64):\n",
    "                x = Conv2D(filters, kernel_size=3, strides=1, padding='same')(input)\n",
    "                x = BatchNormalization(momentum=0.8)(x)\n",
    "                x = PReLU(shared_axes=[1,2])(x)            \n",
    "                x = Conv2D(filters, kernel_size=3, strides=1, padding='same')(x)\n",
    "                x = BatchNormalization(momentum=0.8)(x)\n",
    "                x = Add()([x, input])\n",
    "                return x\n",
    "\n",
    "            filters = 32\n",
    "            x = Conv2D(filters, kernel_size=3, strides=2, padding='same')(input_layer)\n",
    "            x = LeakyReLU(alpha=0.2)(x)\n",
    "            x = residual_block(x,filters)\n",
    "            #x = residual_block(x,filters)\n",
    "            x = Conv2D(filters*2, kernel_size=3, strides=2, padding='same')(x)\n",
    "            x = LeakyReLU(alpha=0.2)(x)\n",
    "            #x = residual_block(x, filters*2)\n",
    "            x = residual_block(x, filters*2)\n",
    "            x = Conv2D(filters*4, kernel_size=3, strides=2, padding='same')(x)\n",
    "            x = LeakyReLU(alpha=0.2)(x)\n",
    "            #x = residual_block(x, filters*4)\n",
    "            x = residual_block(x, filters*4)\n",
    "            x = Conv2D(filters*8, kernel_size=3, strides=2, padding='same')(x)\n",
    "            x = LeakyReLU(alpha=0.2)(x)\n",
    "            #x = residual_block(x, filters*8)\n",
    "            x = residual_block(x, filters*8)\n",
    "            x = Conv2D(filters*16, kernel_size=3, strides=2, padding='same')(x)\n",
    "            x = LeakyReLU(alpha=0.2)(x)\n",
    "            #x = residual_block(x, filters*8)\n",
    "            x = residual_block(x, filters*16)\n",
    "            x = Conv2D(filters*32, kernel_size=3, strides=2, padding='same')(x)\n",
    "            x = LeakyReLU(alpha=0.2)(x)\n",
    "            x = Flatten()(x)\n",
    "            x1 = Dense(1, activation='sigmoid')(x)\n",
    "            x2 = Dense(13, activation='sigmoid')(x)\n",
    "            \n",
    "            model = Model(inputs=input_layer, outputs=[x1,x2])\n",
    "            model.summary()\n",
    "            return model,x1,x2\n",
    "    def train(self,X_train, y_train, epochs, batch_size=128, sample_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        #(X_train, y_train), (_, _) = mnist.load_data()\n",
    "\n",
    "        # Configure inputs\n",
    "        #X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "        #X_train = np.expand_dims(X_train, axis=3)\n",
    "        #y_train = y_train.reshape(-1, 1)\n",
    "        #print('y_train',y_train)\n",
    "        \n",
    "        # Adversarial ground truths\n",
    "        #self.generator.load_weights('./saved_model/generator_weights.hdf5')\n",
    "        #self.discriminator.load_weights('./saved_model/discriminator_weights.hdf5')\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for mini_batch in range(X_train.shape[0]//batch_size-1):\n",
    "                # ---------------------\n",
    "                #  Train Discriminator\n",
    "                # ---------------------\n",
    "\n",
    "                # Select a random batch of images\n",
    "                #idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "                idx = np.array([_ for _ in range(batch_size*mini_batch,batch_size*(mini_batch+1))])\n",
    "                imgs = X_train[idx]\n",
    "\n",
    "                # Sample noise as generator input\n",
    "                noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "                #print('imgs',np.shape(imgs))\n",
    "                # The labels of the digits that the generator tries to create an\n",
    "                # image representation of\n",
    "                sampled_labels = np.random.randint(0, self.num_classes, (batch_size, 1))\n",
    "                # Generate a half batch of new images\n",
    "                gen_imgs = self.generator.predict([noise, sampled_labels])\n",
    "                #print('G',np.shape(gen_imgs))\n",
    "                # Image labels. 0-11 if image is valid or 12 if it is generated (fake)\n",
    "                img_labels = y_train[idx]\n",
    "                fake_labels = self.num_classes * np.ones(img_labels.shape)\n",
    "                #print('D',(self.discriminator.predict(gen_imgs)))\n",
    "                # Train the discriminator\n",
    "                if mini_batch % 5 == 0:\n",
    "                    d_loss_real = self.discriminator.train_on_batch(imgs, [valid, img_labels])\n",
    "                    #print('real_loss',d_loss_real)\n",
    "                    d_loss_fake = self.discriminator.train_on_batch(gen_imgs, [fake, fake_labels])\n",
    "                    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "                # ---------------------\n",
    "                #  Train Generator\n",
    "                # ---------------------\n",
    "\n",
    "                # Train the generator\n",
    "                g_loss = self.combined.train_on_batch([noise, sampled_labels], [valid, sampled_labels])\n",
    "\n",
    "                # Plot the progress\n",
    "                print (\"Epoch%d,MiniBatch:%d [D loss: %f, acc.: %.2f%%, op_acc: %.2f%%] [G loss: %f]\" % (epoch,mini_batch, d_loss[0], 100*d_loss[3], 100*d_loss[4], g_loss[0]))\n",
    "\n",
    "                # If at save interval => save generated image samples\n",
    "                if mini_batch % sample_interval == 0 or mini_batch == (X_train.shape[0]//batch_size-1):\n",
    "                    self.save_model()\n",
    "                    self.sample_images(epoch,mini_batch)\n",
    "            \n",
    "\n",
    "    def sample_images(self, epoch,mini_batch):\n",
    "        r, c = 2, 6\n",
    "        noise = np.random.normal(0, 1, (r * c, 100))\n",
    "        sampled_labels = np.array([_ for _ in range(r*c)])\n",
    "        gen_imgs = self.generator.predict([noise, sampled_labels])\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt,:,:,:])\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(\"images/flip_%d_%d.png\" % (epoch,mini_batch))\n",
    "        plt.close()\n",
    "        \n",
    "\n",
    "    def save_model(self):\n",
    "        def save(model, model_name):\n",
    "            model_path = \"saved_model/%s.json\" % model_name\n",
    "            weights_path = \"saved_model/%s_weights.hdf5\" % model_name\n",
    "            options = {\"file_arch\": model_path,\n",
    "                        \"file_weight\": weights_path}\n",
    "            json_string = model.to_json()\n",
    "            open(options['file_arch'], 'w').write(json_string)\n",
    "            model.save_weights(options['file_weight'])\n",
    "\n",
    "        save(self.generator, \"generator\")\n",
    "        save(self.discriminator, \"discriminator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acgan = ACGAN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tags = pd.read_csv('./tags_hair_num.csv')\n",
    "X_train = np.array([(np.array(load_img(\"./faces/{}.jpg\".format(ids))) / 127.5 - 1) for ids in tqdm_notebook(tags.idx)])\n",
    "y_train = np.array(tags.hair)\n",
    "\n",
    "\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "np.shape(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = np.array([np.fliplr(x) for x in X_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "        from keras.datasets import mnist\n",
    "        # Load the dataset\n",
    "        (X_train, y_train), (_, _) = mnist.load_data()\n",
    "\n",
    "        # Configure inputs\n",
    "        X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "        X_train = np.expand_dims(X_train, axis=3)\n",
    "        y_train = y_train.reshape(-1, 1)\n",
    "        #print('y_train',y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acgan.train(X_train, y_train,epochs=50, batch_size=32, sample_interval=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
